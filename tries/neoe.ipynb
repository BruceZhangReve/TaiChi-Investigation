{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "[Taichi] version 1.7.2, llvm 15.0.5, commit 0131dce9, osx, python 3.9.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 03/17/25 16:07:16.732 205289] [shell.py:_shell_pop_print@23] Graphical python shell detected, using wrapped sys.stdout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=x64\n",
      "\n",
      "============================\n",
      "Taichi (J^T v) vs. PyTorch (full Jacobian) (J^T v)\n",
      "============================\n",
      "\n",
      "Sample 0:\n",
      "  Taichi :   [ 0.40772447  0.85730678  0.18844506  0.31153518 -0.23295417 -0.08521497\n",
      " -0.24905279  0.5076037 ]\n",
      "  PyTorch:   [ 0.40772447  0.8573068   0.18844506  0.31153518 -0.23295417 -0.08521497\n",
      " -0.2490528   0.5076037 ]\n",
      "  Max error = 0.000000e+00\n",
      "Sample 1:\n",
      "  Taichi :   [-1.31211734  0.4779858  -0.65214491 -1.96736133 -0.38303438 -0.48829293\n",
      " -0.42431751 -0.01537222]\n",
      "  PyTorch:   [-1.3121173   0.4779858  -0.6521449  -1.9673613  -0.38303438 -0.48829293\n",
      " -0.4243175  -0.01537222]\n",
      "  Max error = 0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "ti.init(arch=ti.cpu)\n",
    "\n",
    "# ---------------- 设置维度 ----------------\n",
    "B = 2   # batch size\n",
    "n = 8   # 输入维度\n",
    "m = 4   # 输出维度\n",
    "\n",
    "# ---------------- 定义 Taichi 变量 ----------------\n",
    "W = ti.Matrix.field(m, n, dtype=ti.f32, shape=())\n",
    "bias = ti.Vector.field(m, dtype=ti.f32, shape=())\n",
    "\n",
    "# x[b] : b号样本的输入向量 (size=n)\n",
    "x = ti.Vector.field(n, dtype=ti.f32, shape=(B,), needs_grad=True)\n",
    "# y[b] : b号样本的输出向量 (size=m)\n",
    "y = ti.Vector.field(m, dtype=ti.f32, shape=(B,), needs_grad=True)\n",
    "\n",
    "# v[b] : 对应每个样本的向量 (size=m), 用于 J^T v 的乘法\n",
    "v = ti.Vector.field(m, dtype=ti.f32, shape=(B,))\n",
    "\n",
    "# 对应每个样本的结果 (J^T v)[b] (size=n)\n",
    "JVP_result = ti.Vector.field(n, dtype=ti.f32, shape=(B,))\n",
    "\n",
    "# ---------------- Kernel：前向传播 (处理 batch 中所有样本) ----------------\n",
    "@ti.kernel\n",
    "def compute_y_batch():\n",
    "    for b_ in range(B):\n",
    "        y[b_] = ti.tanh(W[None] @ x[b_] + bias[None])\n",
    "\n",
    "# ---------------- Kernel：累加第 i 个分量的贡献到 JVP_result ----------------\n",
    "@ti.kernel\n",
    "def accumulate_jvp(i: ti.i32):\n",
    "    for b_ in range(B):\n",
    "        for j in ti.static(range(n)):\n",
    "            JVP_result[b_][j] += x.grad[b_][j] * v[b_][i]\n",
    "\n",
    "# ---------------- (1) 随机初始化数据 ----------------\n",
    "torch.manual_seed(42)  # for reproducibility\n",
    "\n",
    "W_np = torch.randn(m, n, dtype=torch.float32).numpy()\n",
    "bias_np = torch.randn(m, dtype=torch.float32).numpy()\n",
    "x_np = torch.randn(B, n, dtype=torch.float32).numpy()\n",
    "v_np = torch.randn(B, m, dtype=torch.float32).numpy()\n",
    "\n",
    "# 将 NumPy 数组赋值给 Taichi\n",
    "W[None] = W_np\n",
    "bias[None] = bias_np\n",
    "for b_ in range(B):\n",
    "    x[b_] = x_np[b_]\n",
    "    v[b_] = v_np[b_]\n",
    "    JVP_result[b_] = ti.Vector(np.zeros(n, dtype=np.float32))\n",
    "\n",
    "# ---------------- (2) 在 Taichi 中计算 J^T v ----------------\n",
    "for i in range(m):\n",
    "    # 重置梯度\n",
    "    for b_ in range(B):\n",
    "        x.grad[b_] = ti.Vector(np.zeros(n, dtype=np.float32))\n",
    "        y.grad[b_] = ti.Vector(np.zeros(m, dtype=np.float32))\n",
    "\n",
    "    # 对输出的第 i 个分量打上 grad=1 (每个样本都打 1)\n",
    "    for b_ in range(B):\n",
    "        y.grad[b_][i] = 1.0\n",
    "\n",
    "    # 前向 + 反向\n",
    "    compute_y_batch()\n",
    "    compute_y_batch.grad()\n",
    "\n",
    "    # 累加到 JVP_result\n",
    "    accumulate_jvp(i)\n",
    "\n",
    "# 拉取 Taichi 结果到 NumPy\n",
    "taichi_jtv = np.vstack([JVP_result[b_].to_numpy() for b_ in range(B)])  # shape (B, n)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "#       用 PyTorch 的 torch.autograd.functional.jacobian\n",
    "#       构造 full Jacobian，然后与 v 做乘法\n",
    "# -------------------------------------------------------------\n",
    "W_torch = torch.tensor(W_np, dtype=torch.float32)   # shape (m, n)\n",
    "b_torch = torch.tensor(bias_np, dtype=torch.float32)# shape (m,)\n",
    "x_torch = torch.tensor(x_np, dtype=torch.float32)   # shape (B, n), no grad needed for jacobian call\n",
    "v_torch = torch.tensor(v_np, dtype=torch.float32)   # shape (B, m)\n",
    "\n",
    "def forward_fn(x_):\n",
    "    \"\"\"x_.shape = (B, n). Returns (B, m).\"\"\"\n",
    "    return torch.tanh(x_ @ W_torch.T + b_torch)\n",
    "\n",
    "# (3) 计算 full Jacobian: shape = (B, m, B, n)\n",
    "jac = torch.autograd.functional.jacobian(forward_fn, x_torch, create_graph=False)\n",
    "# 正确提取每个样本的 Jacobian 矩阵\n",
    "# 原始 jac 形状为 (B, m, B, n)，其中 jac[b, :, b, :] 对应样本 b 的 Jacobian\n",
    "jac_transposed = jac.transpose(1, 2)  # 形状变为 (B, B, m, n)\n",
    "jac_transposed = jac_transposed.diagonal(dim1=0, dim2=1).movedim(-1, 0)  # 形状 (B, m, n)\n",
    "#print(jac_transposed.shape,v_torch.shape)\n",
    "# 正确计算 J^T v\n",
    "torch_jtv_jacobian = torch.einsum('bmn,bm->bn', jac_transposed, v_torch).numpy()\n",
    "\n",
    "\n",
    "# (4) 做对比\n",
    "print(\"\\n============================\")\n",
    "print(\"Taichi (J^T v) vs. PyTorch (full Jacobian) (J^T v)\")\n",
    "print(\"============================\\n\")\n",
    "for b_ in range(B):\n",
    "    print(f\"Sample {b_}:\")\n",
    "    print(\"  Taichi :  \", taichi_jtv[b_])\n",
    "    print(\"  PyTorch:  \", torch_jtv_jacobian[b_])\n",
    "    err = np.max(np.abs(taichi_jtv[b_] - torch_jtv_jacobian[b_]))\n",
    "    print(f\"  Max error = {err:e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "速度比较\n",
      "============================\n",
      "Taichi 计算时间: 1.88 ms\n",
      "PyTorch 计算时间: 2.86 ms\n",
      "Taichi 比 PyTorch 快 1.53 倍\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# ---------------- (5) 加入速度比较 ----------------\n",
    "# Taichi 计算时间\n",
    "start_time = time.time()\n",
    "for i in range(m):\n",
    "    # 重置梯度\n",
    "    for b_ in range(B):\n",
    "        x.grad[b_] = ti.Vector(np.zeros(n, dtype=np.float32))\n",
    "        y.grad[b_] = ti.Vector(np.zeros(m, dtype=np.float32))\n",
    "\n",
    "    # 对输出的第 i 个分量打上 grad=1 (每个样本都打 1)\n",
    "    for b_ in range(B):\n",
    "        y.grad[b_][i] = 1.0\n",
    "\n",
    "    # 前向 + 反向\n",
    "    compute_y_batch()\n",
    "    compute_y_batch.grad()\n",
    "\n",
    "    # 累加到 JVP_result\n",
    "    accumulate_jvp(i)\n",
    "taichi_time = time.time() - start_time\n",
    "\n",
    "# PyTorch 计算时间\n",
    "start_time = time.time()\n",
    "jac = torch.autograd.functional.jacobian(forward_fn, x_torch, create_graph=False)\n",
    "jac_transposed = jac.transpose(1, 2)  # 形状变为 (B, B, m, n)\n",
    "jac_transposed = jac_transposed.diagonal(dim1=0, dim2=1).movedim(-1, 0)  # 形状 (B, m, n)\n",
    "torch_jtv_jacobian = torch.einsum('bmn,bm->bn', jac_transposed, v_torch).numpy()\n",
    "torch_time = time.time() - start_time\n",
    "\n",
    "# ---------------- (6) 输出速度比较结果 ----------------\n",
    "print(\"\\n============================\")\n",
    "print(\"速度比较\")\n",
    "print(\"============================\")\n",
    "print(f\"Taichi 计算时间: {taichi_time * 1000:.2f} ms\")\n",
    "print(f\"PyTorch 计算时间: {torch_time * 1000:.2f} ms\")\n",
    "print(f\"Taichi 比 PyTorch 快 {torch_time / taichi_time:.2f} 倍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
