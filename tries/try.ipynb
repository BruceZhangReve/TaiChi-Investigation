{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "CPU Time: 11.3047 seconds\n",
      "MPS Time: 1.1047 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.backends.mps.is_available()\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 设备选择\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "device_mps = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 定义模型\n",
    "model_cpu = torch.nn.Linear(1000, 1000).to(device_cpu)\n",
    "model_mps = torch.nn.Linear(1000, 1000).to(device_mps)\n",
    "\n",
    "# 创建输入数据\n",
    "inputs_cpu = torch.randn(1000, 1000).to(device_cpu)\n",
    "inputs_mps = torch.randn(1000, 1000).to(device_mps)\n",
    "\n",
    "# 测试 CPU 时间\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    outputs_cpu = model_cpu(inputs_cpu)\n",
    "cpu_time = time.time() - start_time\n",
    "print(f\"CPU Time: {cpu_time:.4f} seconds\")\n",
    "\n",
    "# 测试 MPS 时间\n",
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "    outputs_mps = model_mps(inputs_mps)\n",
    "torch.mps.synchronize()  # 确保 MPS 完成计算\n",
    "mps_time = time.time() - start_time\n",
    "print(f\"MPS Time: {mps_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "tensor(4.)\n",
      "tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "z = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = x ** 2 + z ** 3\n",
    "\n",
    "# Calculate dy/dx and dy/dz\n",
    "grad_x, grad_z = torch.autograd.grad(y, [x, z], grad_outputs=torch.tensor(1.0))\n",
    "print(grad_x)\n",
    "print(grad_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(12.),)\n"
     ]
    }
   ],
   "source": [
    "#Second Derevative\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 3 \n",
    "\n",
    "grad_x = torch.autograd.grad(y, x, create_graph=True)\n",
    "#Note, create_graphs allows to calculate 2nd-derevative, other wise torch will stop tracking gradient\n",
    "grad2_x = torch.autograd.grad(grad_x, x)\n",
    "\n",
    "print(grad2_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "z = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = x ** 2  \n",
    "\n",
    "# dy/dx dy/dz\n",
    "#grad_x, grad_z = torch.autograd.grad(y, [x, z])#, allow_unused=True)\n",
    "grad_x, grad_z = torch.autograd.grad(y, [x, z], allow_unused=True)\n",
    "#If y not depend on some variable, we need to set allow_unused=True so that no error occur\n",
    "\n",
    "print(grad_x)  \n",
    "print(grad_z)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.autograd.functional.jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  0.],\n",
      "        [ 0., 27.],\n",
      "        [ 3.,  2.]])\n"
     ]
    }
   ],
   "source": [
    "# f: R^2 -> R^3 | y(x) = (y1(x1,x2),y2(x1,x2),y3(x1,x2))\n",
    "\n",
    "#This is a wrong method, if return a new tensor, then the computation graph is not tracked\n",
    "#def func(x):\n",
    "    #return torch.tensor([\n",
    "        #x[0] ** 2,   # y1(x1,x2) = x1^2\n",
    "        #x[1] ** 3,   # y2(x1,x2) = x2^3\n",
    "        #x[0] * x[1]  # y3(x1,x2) = x1 * x2\n",
    "    #])\n",
    "\n",
    "def func(x):\n",
    "    return torch.stack([\n",
    "        x[0] ** 2,   # y1(x1, x2) = x1^2\n",
    "        x[1] ** 3,   # y2(x1, x2) = x2^3\n",
    "        x[0] * x[1]  # y3(x1, x2) = x1 * x2\n",
    "    ])\n",
    "\n",
    "\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Compute Jacobian\n",
    "J = torch.autograd.functional.jacobian(func, x)\n",
    "\n",
    "print(J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taichi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=metal\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m gui\u001b[38;5;241m.\u001b[39mrunning:\n\u001b[1;32m     28\u001b[0m     paint(i \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.03\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mgui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     gui\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     31\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ppi/lib/python3.11/site-packages/taichi/ui/gui.py:319\u001b[0m, in \u001b[0;36mGUI.set_image\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;66;03m# Type matched! We can use an optimized copy kernel.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m img\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage resolution does not match GUI resolution\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m         \u001b[43mtensor_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m         ti\u001b[38;5;241m.\u001b[39msync()\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, taichi\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mmatrix\u001b[38;5;241m.\u001b[39mMatrixField):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ppi/lib/python3.11/site-packages/taichi/lang/kernel_impl.py:1113\u001b[0m, in \u001b[0;36m_kernel_impl.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(_func)\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1112\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (TaichiCompilationError, TaichiRuntimeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mget_runtime()\u001b[38;5;241m.\u001b[39mprint_full_traceback:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ppi/lib/python3.11/site-packages/taichi/lang/shell.py:27\u001b[0m, in \u001b[0;36m_shell_pop_print.<locals>.new_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(old_call)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 27\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mold_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print's in kernel won't take effect until ti.sync(), discussion:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# https://github.com/taichi-dev/taichi/pull/1303#discussion_r444897102\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_ti_core\u001b[38;5;241m.\u001b[39mpop_python_print_buffer(), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ppi/lib/python3.11/site-packages/taichi/lang/kernel_impl.py:1045\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensure_compiled(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1044\u001b[0m kernel_cpp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_kernels[key]\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_cpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ppi/lib/python3.11/site-packages/taichi/lang/kernel_impl.py:971\u001b[0m, in \u001b[0;36mKernel.launch_kernel\u001b[0;34m(self, t_kernel, *args)\u001b[0m\n\u001b[1;32m    969\u001b[0m     compiled_kernel_data \u001b[38;5;241m=\u001b[39m prog\u001b[38;5;241m.\u001b[39mcompile_kernel(prog\u001b[38;5;241m.\u001b[39mconfig(), prog\u001b[38;5;241m.\u001b[39mget_device_caps(), t_kernel)\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;66;03m# Launch kernel\u001b[39;00m\n\u001b[0;32m--> 971\u001b[0m     \u001b[43mprog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiled_kernel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaunch_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     e \u001b[38;5;241m=\u001b[39m handle_exception_from_cpp(e)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\n",
    "ti.init(arch=ti.gpu)\n",
    "\n",
    "n = 320\n",
    "pixels = ti.field(dtype=float, shape=(n * 2, n))\n",
    "\n",
    "\"\"\"\n",
    "@ti.func\n",
    "def complex_sqr(z):  # complex square of a 2D vector\n",
    "    return tm.vec2(z[0] * z[0] - z[1] * z[1], 2 * z[0] * z[1])\n",
    "\n",
    "@ti.kernel\n",
    "def paint(t: float):\n",
    "    for i, j in pixels:  # Parallelized over all pixels\n",
    "        c = tm.vec2(-0.8, tm.cos(t) * 0.2)\n",
    "        z = tm.vec2(i / n - 1, j / n - 0.5) * 2\n",
    "        iterations = 0\n",
    "        while z.norm() < 20 and iterations < 50:\n",
    "            z = complex_sqr(z) + c\n",
    "            iterations += 1\n",
    "        pixels[i, j] = 1 - iterations * 0.02\n",
    "\n",
    "gui = ti.GUI(\"Julia Set\", res=(n * 2, n))\n",
    "\n",
    "i = 0\n",
    "while gui.running:\n",
    "    paint(i * 0.03)\n",
    "    gui.set_image(pixels)\n",
    "    gui.show()\n",
    "    i += 1\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=metal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "taichi.lang.matrix.Vector"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import taichi as ti\n",
    "import taichi.math as tm\n",
    "\n",
    "ti.init(arch=ti.gpu)\n",
    "\n",
    "n = 320\n",
    "pixels = ti.field(dtype=float, shape=(n * 2, n))\n",
    "\n",
    "type(tm.vec2(-0.8, tm.cos(0) * 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that: \n",
    "#For loops located at the outermost scope in a Taichi kernel are automatically parallelized.\n",
    "\n",
    "@ti.kernel\n",
    "def foo():\n",
    "    for i in x:\n",
    "        ...\n",
    "        break # Error!\n",
    "\n",
    "@ti.kernel\n",
    "def foo():\n",
    "    for i in x:\n",
    "        for j in range(10):\n",
    "            ...\n",
    "            break # OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taichi 4 Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78498\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Count the prime numbers in the range [1, n], via traditional python code\"\"\"\n",
    "\n",
    "# Checks if a positive integer is a prime number\n",
    "def is_prime(n: int):\n",
    "    result = True\n",
    "    # Traverses the range between 2 and sqrt(n)\n",
    "    # - Returns False if n can be divided by one of them;\n",
    "    # - otherwise, returns True\n",
    "    for k in range(2, int(n ** 0.5) + 1):\n",
    "        if n % k == 0:\n",
    "            result = False\n",
    "            break\n",
    "    return result\n",
    "\n",
    "# Traverses the range between 2 and n\n",
    "# Counts the primes according to the return of is_prime()\n",
    "def count_primes(n: int) -> int:\n",
    "    count = 0\n",
    "    for k in range(2, n):\n",
    "        if is_prime(k):\n",
    "           count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "print(count_primes(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=x64\n",
      "78498\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "ti.init(arch=ti.cpu)\n",
    "\n",
    "@ti.func\n",
    "def is_prime(n: int):\n",
    "    result = True\n",
    "    for k in range(2, int(n ** 0.5) + 1):\n",
    "        if n % k == 0:\n",
    "            result = False\n",
    "            break\n",
    "    return result\n",
    "\n",
    "@ti.kernel\n",
    "def count_primes(n: int) -> int:\n",
    "    count = 0\n",
    "    for k in range(2, n): #This outerloop is parallelized\n",
    "        if is_prime(k):\n",
    "            count += 1 # Will this be an issue? Using the same variable? \n",
    "\n",
    "    return count\n",
    "\n",
    "\n",
    "print(count_primes(1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taichi for Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y(t)= ∑_{u=0}^{u=t}w(T−1−(t−u))⋅k(u+T−1)+ϵ$$\n",
    "\n",
    "w is convolution kernel, k is inputing sequence, ϵ is a bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_formula_very_slow(w, k, B, C, T, eps):\n",
    "    out = torch.empty((B, C, T), device='cpu')\n",
    "    for b in range(B):   \n",
    "        for c in range(C): \n",
    "            for t in range(T):  \n",
    "                s = eps  \n",
    "                for u in range(t-T+1, t+1):  \n",
    "                    s += w[c][0][(T-1)-(t-u)] * k[b][c][u+T-1]\n",
    "                out[b][c][t] = s\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ti.kernel\n",
    "def taichi_forward_v0(\n",
    "        out: ti.types.ndarray(ndim=3),# type: ignore\n",
    "        w: ti.types.ndarray(ndim=3), # type: ignore\n",
    "        k: ti.types.ndarray(ndim=3),# type: ignore\n",
    "        eps: ti.f32):# type: ignore\n",
    "\n",
    "    for b, c, t in out:  # Parallelize\n",
    "        s = eps\n",
    "        for u in range(t-T+1, t+1):  # Convolution\n",
    "            s += w[c, 0, (T-1)-(t-u)] * k[b, c, u+T-1]\n",
    "        out[b, c, t] = s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "B, C, T = 2, 3, 5  # 2 batch, 3 channle, every sequence has length 5\n",
    "x = torch.rand(B, C, T)  # input\n",
    "w = torch.rand(C, 1, 3)  # convolution kernel (input channel=3, input channel=1, Kernel size=3)\n",
    "\n",
    "y = F.conv1d(x, w, groups=C)  # groups=C: ensure every channel calculate individually??\n",
    "\n",
    "print(y.shape)  # (2, 3, 3) - (B, C, T_out), since no padding losing 2 token of length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Taichi] Starting on arch=x64\n"
     ]
    }
   ],
   "source": [
    "import taichi as ti\n",
    "\n",
    "ti.init(arch=ti.cpu)\n",
    "\n",
    "B, C, T = 2, 3, 5\n",
    "T_filter = 3  \n",
    "eps = 0.1  \n",
    "\n",
    "\n",
    "w = ti.ndarray(dtype=ti.f32, shape=(C, 1, T_filter))  # convolution kernel\n",
    "k = ti.ndarray(dtype=ti.f32, shape=(B, C, T + T_filter - 1))  # input\n",
    "out = ti.ndarray(dtype=ti.f32, shape=(B, C, T))  # out\n",
    "\n",
    "@ti.kernel\n",
    "def taichi_forward_v0(\n",
    "        out: ti.types.ndarray(ndim=3),\n",
    "        w: ti.types.ndarray(ndim=3),\n",
    "        k: ti.types.ndarray(ndim=3),\n",
    "        eps: ti.f32):\n",
    "\n",
    "    for b, c, t in out:  \n",
    "        s = eps\n",
    "        for u in range(t - T_filter + 1, t + 1): \n",
    "            if 0 <= u + T_filter - 1 < k.shape[2]: \n",
    "                s += w[c, 0, (T_filter - 1) - (t - u)] * k[b, c, u + T_filter - 1]\n",
    "        out[b, c, t] = s\n",
    "\n",
    "taichi_forward_v0(out, w, k, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable Proramming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
